{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ac6226",
   "metadata": {},
   "source": [
    "# üé• WAN 2.1 Video Generation Demo\n",
    "\n",
    "This notebook demonstrates how to use the integrated WAN 2.1 text-to-video generation system with our batch processing framework.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- Loading WAN 2.1 model components\n",
    "- Configuring the pipeline for optimal performance\n",
    "- Generating videos from text prompts\n",
    "- Integrating with our project's batch processing system\n",
    "- Exporting and displaying results\n",
    "\n",
    "## Requirements:\n",
    "- CUDA-capable GPU (recommended)\n",
    "- WAN model dependencies installed\n",
    "- At least 16GB VRAM for high-quality generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "from diffusers import AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from diffusers.utils import export_to_video\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import os\n",
    "import hashlib\n",
    "from enum import Enum\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Import our project modules\n",
    "from src.config import ConfigManager, GenerationConfig\n",
    "from src.generators import WAN13BVideoGenerator\n",
    "from src.utils import FileManager, ProgressTracker\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ae7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Model Configuration\n",
    "model_id = \"Wan-AI/Wan2.1-T2V-14B-Diffusers\"\n",
    "\n",
    "# Configuration settings\n",
    "config = {\n",
    "    \"model_id\": model_id,\n",
    "    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"vae_dtype\": torch.float32,\n",
    "    \n",
    "    # Video settings - choose resolution based on your VRAM\n",
    "    \"width\": 512,      # Use 1280 for 720P (requires more VRAM)\n",
    "    \"height\": 512,     # Use 720 for 720P\n",
    "    \"fps\": 24,\n",
    "    \"duration\": 4.0,   # seconds\n",
    "    \n",
    "    # Generation settings\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Flow shift: 5.0 for 720P, 3.0 for 480P\n",
    "flow_shift = 5.0 if max(config[\"width\"], config[\"height\"]) >= 720 else 3.0\n",
    "config[\"flow_shift\"] = flow_shift\n",
    "\n",
    "print(\"üéõÔ∏è  Model Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nüì∫ Video will be: {config['width']}x{config['height']} at {config['fps']} FPS\")\n",
    "print(f\"‚è±Ô∏è  Duration: {config['duration']} seconds ({int(config['fps'] * config['duration'])} frames)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wan Model Components\n",
    "print(\"üîÑ Loading WAN model components...\")\n",
    "\n",
    "# Load VAE (Video AutoEncoder)\n",
    "print(\"   üì¶ Loading VAE...\")\n",
    "vae = AutoencoderKLWan.from_pretrained(\n",
    "    config[\"model_id\"], \n",
    "    subfolder=\"vae\", \n",
    "    torch_dtype=config[\"vae_dtype\"]\n",
    ")\n",
    "print(\"   ‚úÖ VAE loaded successfully\")\n",
    "\n",
    "# Create scheduler with flow prediction\n",
    "print(\"   ‚öôÔ∏è  Creating scheduler...\")\n",
    "scheduler = UniPCMultistepScheduler(\n",
    "    prediction_type='flow_prediction',\n",
    "    use_flow_sigmas=True,\n",
    "    num_train_timesteps=1000,\n",
    "    flow_shift=config[\"flow_shift\"]\n",
    ")\n",
    "print(f\"   ‚úÖ UniPCMultistepScheduler created (flow_shift={config['flow_shift']})\")\n",
    "\n",
    "print(\"üéâ Model components loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22045b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Pipeline Settings\n",
    "print(\"üöÄ Initializing WAN pipeline...\")\n",
    "\n",
    "# Load the complete pipeline\n",
    "pipe = WanPipeline.from_pretrained(\n",
    "    config[\"model_id\"],\n",
    "    vae=vae,\n",
    "    torch_dtype=config[\"torch_dtype\"]\n",
    ")\n",
    "\n",
    "# Set our custom scheduler\n",
    "pipe.scheduler = scheduler\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   üéÆ Moving pipeline to {config['device']}\")\n",
    "    pipe.to(config[\"device\"])\n",
    "    \n",
    "    # Enable memory efficient attention if available\n",
    "    try:\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"   ‚ö° XFormers memory efficient attention enabled\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  XFormers not available, using default attention\")\n",
    "        \n",
    "    # Enable CPU offload for large models if needed\n",
    "    # pipe.enable_sequential_cpu_offload()  # Uncomment if you have limited VRAM\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  CUDA not available, using CPU (very slow)\")\n",
    "\n",
    "print(\"‚úÖ Pipeline configured and ready for generation!\")\n",
    "\n",
    "# Print memory usage if on CUDA\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"üíæ GPU Memory - Allocated: {memory_allocated:.1f}GB, Reserved: {memory_reserved:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Video from Text Prompt\n",
    "prompt = \"a romantic kiss between two people at sunset\"\n",
    "print(f\"üé¨ Generating video for prompt: '{prompt}'\")\n",
    "\n",
    "# Create generator for reproducible results\n",
    "generator = torch.Generator(device=config[\"device\"]).manual_seed(config[\"seed\"])\n",
    "\n",
    "# Calculate number of frames\n",
    "num_frames = int(config[\"fps\"] * config[\"duration\"])\n",
    "print(f\"üìä Generation parameters:\")\n",
    "print(f\"   Resolution: {config['width']}x{config['height']}\")\n",
    "print(f\"   Frames: {num_frames} ({config['duration']}s at {config['fps']} FPS)\")\n",
    "print(f\"   Steps: {config['num_inference_steps']}\")\n",
    "print(f\"   Guidance: {config['guidance_scale']}\")\n",
    "print(f\"   Seed: {config['seed']}\")\n",
    "\n",
    "# Start generation\n",
    "start_time = time.time()\n",
    "print(f\"\\n‚è≥ Starting generation... (this may take several minutes)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    video_frames = pipe(\n",
    "        prompt=prompt,\n",
    "        width=config[\"width\"],\n",
    "        height=config[\"height\"],\n",
    "        num_frames=num_frames,\n",
    "        num_inference_steps=config[\"num_inference_steps\"],\n",
    "        guidance_scale=config[\"guidance_scale\"],\n",
    "        generator=generator\n",
    "    ).frames[0]\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"‚úÖ Video generated successfully!\")\n",
    "print(f\"‚è±Ô∏è  Generation time: {generation_time:.1f} seconds\")\n",
    "print(f\"üì¶ Generated {len(video_frames)} frames\")\n",
    "\n",
    "# Clear some GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"üíæ GPU Memory after generation: {memory_allocated:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Export Video Output\n",
    "print(\"üíæ Saving video to file...\")\n",
    "\n",
    "# Create output directory structure using our project's file manager\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(f\"outputs/notebook_generation_{timestamp}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export video\n",
    "video_path = output_dir / \"generated_video.mp4\"\n",
    "export_to_video(video_frames, str(video_path), fps=config[\"fps\"])\n",
    "\n",
    "print(f\"üé• Video saved to: {video_path}\")\n",
    "print(f\"üìè File size: {video_path.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "# Save generation metadata\n",
    "metadata = {\n",
    "    \"prompt\": prompt,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"generation_time\": generation_time,\n",
    "    \"config\": config,\n",
    "    \"model_info\": {\n",
    "        \"model_id\": config[\"model_id\"],\n",
    "        \"pipeline_type\": \"WanPipeline\",\n",
    "        \"scheduler\": \"UniPCMultistepScheduler\"\n",
    "    },\n",
    "    \"video_info\": {\n",
    "        \"path\": str(video_path),\n",
    "        \"frames\": len(video_frames),\n",
    "        \"duration\": config[\"duration\"],\n",
    "        \"fps\": config[\"fps\"],\n",
    "        \"resolution\": f\"{config['width']}x{config['height']}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = output_dir / \"metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìã Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Also save the prompt for easy reference\n",
    "prompt_path = output_dir / \"prompt.txt\"\n",
    "with open(prompt_path, 'w') as f:\n",
    "    f.write(prompt)\n",
    "\n",
    "print(f\"üìù Prompt saved to: {prompt_path}\")\n",
    "print(f\"\\nüìÅ Complete output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd194f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Generated Video\n",
    "print(\"üñºÔ∏è  Displaying video frames...\")\n",
    "\n",
    "# Create a grid to show some frames\n",
    "num_display_frames = min(8, len(video_frames))\n",
    "frames_to_show = [video_frames[i] for i in range(0, len(video_frames), len(video_frames) // num_display_frames)][:num_display_frames]\n",
    "\n",
    "# Setup matplotlib figure\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, frame in enumerate(frames_to_show):\n",
    "    if i < len(axes):\n",
    "        # Convert frame to numpy array if needed\n",
    "        frame_array = np.array(frame)\n",
    "        axes[i].imshow(frame_array)\n",
    "        axes[i].set_title(f\"Frame {i * len(video_frames) // num_display_frames}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(frames_to_show), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Generated Video Frames - '{prompt}'\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üì∫ Showing {len(frames_to_show)} sample frames from the {len(video_frames)}-frame video\")\n",
    "print(f\"üéûÔ∏è  Full video saved as: {video_path.name}\")\n",
    "\n",
    "# Display video info\n",
    "print(f\"\\nüìä Video Statistics:\")\n",
    "print(f\"   ‚Ä¢ Resolution: {config['width']} x {config['height']} pixels\")\n",
    "print(f\"   ‚Ä¢ Duration: {config['duration']} seconds\")\n",
    "print(f\"   ‚Ä¢ Frame Rate: {config['fps']} FPS\") \n",
    "print(f\"   ‚Ä¢ Total Frames: {len(video_frames)}\")\n",
    "print(f\"   ‚Ä¢ Generation Time: {generation_time:.1f} seconds\")\n",
    "print(f\"   ‚Ä¢ Time per Frame: {generation_time/len(video_frames):.2f} seconds\")\n",
    "\n",
    "# Note about playing the video\n",
    "print(f\"\\nüí° To play the video:\")\n",
    "print(f\"   ‚Ä¢ Open: {video_path}\")\n",
    "print(f\"   ‚Ä¢ Or use: python -c 'import cv2; cv2.VideoCapture(\\\"{video_path}\\\").read()'\")\n",
    "print(f\"   ‚Ä¢ Or any video player that supports MP4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with Batch Processing System\n",
    "print(\"üîÑ Demonstrating integration with our batch processing system...\")\n",
    "\n",
    "# Load our project configuration\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "try:\n",
    "    # Load the default configuration\n",
    "    project_config = config_manager.load_config(\"configs/default.yaml\")\n",
    "    print(\"‚úÖ Project configuration loaded successfully\")\n",
    "    \n",
    "    # Test the WAN generator from our system\n",
    "    wan_generator = WAN13BVideoGenerator(project_config)\n",
    "    \n",
    "    if wan_generator.is_real_model():\n",
    "        print(\"üéâ Real WAN model is being used by our system!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  System is using mock implementation (WAN dependencies not available)\")\n",
    "    \n",
    "    print(f\"üîß System configuration:\")\n",
    "    print(f\"   Model ID: {project_config.model_settings.model_id}\")\n",
    "    print(f\"   Sampler: {project_config.model_settings.sampler}\")\n",
    "    print(f\"   Resolution: {project_config.video_settings.width}x{project_config.video_settings.height}\")\n",
    "    print(f\"   Videos per variation: {project_config.videos_per_variation}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading project config: {e}\")\n",
    "\n",
    "print(f\"\\nüí° To use our batch processing system:\")\n",
    "print(f'   python main.py --template \"a [romantic|passionate] kiss\" --videos-per-variation 2')\n",
    "print(f'   python main.py --config configs/high_quality.yaml --template \"your prompt template\"')\n",
    "print(f'   python main.py --preview --template \"test [this|that] template\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Prompt Variations (Quick Example)\n",
    "print(\"üé≠ Quick demonstration of prompt variations...\")\n",
    "\n",
    "# Example prompts with variations\n",
    "prompt_templates = [\n",
    "    \"a [gentle|passionate] kiss between [two people|a couple]\",\n",
    "    \"a [cute|playful] [cat|dog] in a [garden|house]\",\n",
    "    \"a [dramatic|serene] [sunset|sunrise] over [mountains|ocean]\"\n",
    "]\n",
    "\n",
    "from src.prompts import PromptTemplate\n",
    "\n",
    "for i, template in enumerate(prompt_templates):\n",
    "    print(f\"\\nüìù Template {i+1}: {template}\")\n",
    "    \n",
    "    prompt_template = PromptTemplate(template)\n",
    "    variations = prompt_template.generate_variations()\n",
    "    \n",
    "    print(f\"   Generates {len(variations)} variations:\")\n",
    "    for j, variation in enumerate(variations[:4]):  # Show first 4\n",
    "        print(f\"     {j+1}. {variation.text}\")\n",
    "    \n",
    "    if len(variations) > 4:\n",
    "        print(f\"     ... and {len(variations) - 4} more\")\n",
    "\n",
    "print(f\"\\nüöÄ To generate videos for all variations:\")\n",
    "print(f'   python main.py --template \"a [gentle|passionate] kiss\" --videos-per-variation 2')\n",
    "print(f\"   This would create {len(PromptTemplate('a [gentle|passionate] kiss').generate_variations())} √ó 2 = {len(PromptTemplate('a [gentle|passionate] kiss').generate_variations()) * 2} videos total!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa782f",
   "metadata": {},
   "source": [
    "## üéâ Demo Complete!\n",
    "\n",
    "### What we accomplished:\n",
    "- ‚úÖ **Loaded WAN 2.1 model** with proper VAE and scheduler configuration\n",
    "- ‚úÖ **Generated a video** from text prompt using the real model\n",
    "- ‚úÖ **Saved video and metadata** with proper organization\n",
    "- ‚úÖ **Integrated with our batch system** for scalable processing\n",
    "- ‚úÖ **Demonstrated prompt variations** for systematic content generation\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Install Dependencies:**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. **Run Batch Generation:**\n",
    "   ```bash\n",
    "   # Preview what will be generated\n",
    "   python main.py --preview --template \"a [romantic|dramatic] scene with [two people|a couple]\"\n",
    "   \n",
    "   # Generate with default settings\n",
    "   python main.py --template \"your template here\"\n",
    "   \n",
    "   # High-quality generation\n",
    "   python main.py --config configs/high_quality.yaml --template \"your template\"\n",
    "   ```\n",
    "\n",
    "3. **Customize Settings:**\n",
    "   - Edit `configs/default.yaml` for your preferred settings\n",
    "   - Adjust resolution based on your VRAM (512x512 vs 1280x720)\n",
    "   - Modify generation parameters (steps, guidance, etc.)\n",
    "\n",
    "### üí° Tips for Best Results:\n",
    "\n",
    "- **Memory Management:** Use 512x512 for lower VRAM, 720P for high-end GPUs\n",
    "- **Quality vs Speed:** More steps = better quality but slower generation\n",
    "- **Batch Processing:** Use our system for generating many variations efficiently\n",
    "- **Organization:** All outputs are automatically organized with metadata\n",
    "\n",
    "### üîß System Integration:\n",
    "The WAN model is now fully integrated into our batch processing system. You can use either:\n",
    "- **This notebook** for interactive single-video generation\n",
    "- **Command line tool** for automated batch processing with prompt variations\n",
    "\n",
    "Both approaches use the same underlying WAN model implementation!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
